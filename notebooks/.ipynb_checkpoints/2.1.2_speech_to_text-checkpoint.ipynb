{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec552ce-000e-45af-8106-fe339b175d07",
   "metadata": {},
   "source": [
    "# Performing speech-to-text on files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39e82174-8609-4f94-b35b-cbbd0c08e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16f27672-9d1a-47a9-b3e0-7e2f73d60ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import os\n",
    "import whisper\n",
    "import time\n",
    "\n",
    "import speech_recognition as sr\n",
    "# import cloudstorage as gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd6dc218-d61a-4f4c-87cb-bcfa77c260e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure user\n",
    "user = \"mitchel\"\n",
    "extracted_feats = True\n",
    "\n",
    "if user.lower() == \"jonas\":\n",
    "    BASE_PATH = Path(\"/users/jonvdrdo/jonas/data/aaa_contextaware/raw/uz_study/\")\n",
    "elif user.lower() == \"mitchel\":\n",
    "#     BASE_PATH = Path(\"Z:/shares/ghep_lab/2021_VanhollebekeKappen_EEGStudy2_MIST_Cyberball_Audio/\")\n",
    "    BASE_PATH = Path(\"E:/Data/EEG_Study_2\")\n",
    "DATA_PATH = BASE_PATH.joinpath(\"Data/Raw/Audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a034397d-ca1e-4ecd-8fc4-22943a68af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(DATA_PATH)\n",
    "# os.listdir()\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "efcd7a83-229b-402d-8ee5-e7f222f26e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ppt_31_cybb_speech/audio_referential_stress.wav'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(whisper.transcribe)\n",
    "import os, random\n",
    "random.choice(os.listdir())\n",
    "\n",
    "allFileNames = [\"audio_picture_baseline.wav\", \"audio_picture_control.wav\", \"audio_picture_stress.wav\", \"audio_referential_control.wav\", \"audio_referential_stress.wav\"]\n",
    "filedir = random.choice(os.listdir())\n",
    "audiofiledir = random.choice(allFileNames)\n",
    "\n",
    "filedir + '/' + audiofiledir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f79f2a8a-23d6-411e-bb0e-bc3da5bd2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.thepythoncode.com/article/using-speech-recognition-to-convert-speech-to-text-python\n",
    "fname = \"E:/Data/EEG_Study_2/Data/Raw/Audio/ppt_8_mist_speech/audio_referential_control.wav\"\n",
    "\n",
    "# open the file\n",
    "model = whisper.load_model(\"base\")\n",
    "# result = model.transcribe(\"audio_referential_control.wav\")\n",
    "# print(result[\"text\"])\n",
    "for files in os.listdir():\n",
    "    print(files)\n",
    "    result = model.transcribe(files)\n",
    "    print(result[\"text\"])\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449c2d3-6ea3-4314-87fc-bfeff70789d6",
   "metadata": {},
   "source": [
    "# Test quality/time tradeoff between different model sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cfd24aba-6003-400b-bdee-5a9ab69ef862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running: ppt_13_mist_speech/audio_referential_control.wav\n",
      "Currently running: tiny\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mitch\\anaconda3\\envs\\stress_study_lib\\lib\\site-packages\\whisper\\transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26836/2834159533.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mlanguage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"language\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mpptdir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiledir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudiofiledir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'PptDir'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpptdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'FileName'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Modeltype'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Transcripts'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtranscripts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TimeElapsed'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtimings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Language'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "allFileNames = [\"audio_picture_baseline.wav\", \"audio_picture_control.wav\", \"audio_picture_stress.wav\", \"audio_referential_control.wav\", \"audio_referential_stress.wav\"]\n",
    "modeltypes = [\"tiny\",\"base\",\"small\",\"medium\"] # Different model types we will loop over\n",
    "# Pre-create the lists where we'll store information\n",
    "models = []\n",
    "transcripts = []\n",
    "timings = []\n",
    "language = []\n",
    "pptdir = []\n",
    "\n",
    "filenumberCount = 30\n",
    "\n",
    "for x in range(0,30):\n",
    "    filedir = random.choice(os.listdir())\n",
    "    audiofiledir = random.choice(allFileNames)\n",
    "    print(\"Currently running: \" + filedir + '/' + audiofiledir)\n",
    "    \n",
    "    for modeltype in modeltypes:\n",
    "        print(\"Currently running: \" + modeltype)\n",
    "        model = whisper.load_model(modeltype)\n",
    "        t = time.time()\n",
    "        result = model.transcribe(filedir + '/' + audiofiledir)\n",
    "        elapsed = time.time() - t\n",
    "\n",
    "        models.append(modeltype)\n",
    "        transcripts.append(result[\"text\"])\n",
    "        timings.append(elapsed)\n",
    "        language.append(result[\"language\"])\n",
    "        pptdir.append(filedir)\n",
    "        filename.append(audiofiledir)\n",
    "    \n",
    "df = pd.DataFrame({'PptDir': pptdir, 'FileName': filename, 'Modeltype': models, 'Transcripts': transcripts, 'TimeElapsed': timings, 'Language': language})\n",
    "df\n",
    "\n",
    "df.to_csv('C:/Users/mitch/OneDrive - UGent/Documents/GitHub/stress_cyberball-mist/transcripts/transcriptModelTest2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b909c8-12da-4b43-8e36-c7657d58b303",
   "metadata": {},
   "source": [
    "# Older code ahead, above is final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d993e-907b-4c38-9053-a9e21508a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltypes = [\"tiny\",\"base\",\"small\",\"medium\"] # Different model types we will loop over\n",
    "# Pre-create the lists where we'll store information\n",
    "models = []\n",
    "transcripts = []\n",
    "timings = []\n",
    "language = []\n",
    "\n",
    "for modeltype in modeltypes:\n",
    "    print(\"Currently running: \" + modeltype)\n",
    "    model = whisper.load_model(modeltype)\n",
    "    t = time.time()\n",
    "    result = model.transcribe(\"audio_referential_control.wav\")\n",
    "    elapsed = time.time() - t\n",
    "    \n",
    "    models.append(modeltype)\n",
    "    transcripts.append(result[\"text\"])\n",
    "    timings.append(elapsed)\n",
    "    language.append(result[\"language\"])\n",
    "    print(\"Done running: \" + modeltype)\n",
    "    \n",
    "df = pd.DataFrame({'Modeltype': models, 'Transcripts': transcripts, 'TimeElapsed': timings, 'Language': language})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9a685d7d-2ace-4b34-a281-1ca2c82a9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note to self: For small, a pink progress bar appeared while loading. [only first time, not second time, weird]\n",
    "#   After this was finished, it stayed there for a long time. Unsure what it was doing. \n",
    "# Crashes on 'large', so possibly not supported for Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "692adc04-6f70-4132-9ded-9933c8dc2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users/mitch/OneDrive - UGent/Documents/GitHub/stress_cyberball-mist/transcripts/transcriptModelTest1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "da9e383a-f442-4e48-a526-38f2d881b193",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'segments'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26836/3197306463.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'segments'"
     ]
    }
   ],
   "source": [
    "result.segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cceaaf-a6a7-491d-af59-1b877e9d374e",
   "metadata": {},
   "source": [
    "# This is a next test; windows not relying on information in last window\n",
    "## Might work well, since the text can be pretty random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e7e5804-d9d5-4220-b796-8ce6cab8d9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modeltype</th>\n",
       "      <th>Transcripts</th>\n",
       "      <th>TimeElapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiny</td>\n",
       "      <td>Op de afbilding is een afbilding van Rieken. ...</td>\n",
       "      <td>46.84961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Modeltype                                        Transcripts  TimeElapsed\n",
       "0      tiny   Op de afbilding is een afbilding van Rieken. ...     46.84961"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for modeltype in modeltypes:\n",
    "    model = whisper.load_model(modeltype)\n",
    "    t = time.time()\n",
    "    result = model.transcribe(\"audio_referential_control.wav\", condition_on_previous_text = False)\n",
    "    elapsed = time.time() - t\n",
    "    \n",
    "    models.append(modeltype)\n",
    "    transcripts.append(result[\"text\"])\n",
    "    timings.append(elapsed)\n",
    "    break\n",
    "    \n",
    "df = pd.DataFrame({'Modeltype': models, 'Transcripts': transcripts, 'TimeElapsed': timings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e23a76f-b4e9-4d32-be7f-4ac4e1ceb2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ik zie op het scherm en rekenopgaven staan die staat op het bovenste deel van het scherm aan de rest van het scherm is wit de rekenopgaven omvat een deling eerst en vooral die tussen haakjes staat 72/2 daarna en daarna 2 aftrekkingen en met 27 en -4 sterren is gelijk aan teken en een vraagteken waarde het antwoord zou moeten komen de rekenopgaven is een vrij moeilijke rekenopgave aan moeilijker dan ik verwacht had om hier te zien te krijgen\n"
     ]
    }
   ],
   "source": [
    "# https://www.thepythoncode.com/article/using-speech-recognition-to-convert-speech-to-text-python\n",
    "filename = \"D:/Data/EEG_Study_2/Data/Raw/Audio/ppt_12_mist_speech/audio_referential_control.wav\"\n",
    "\n",
    "# initialize the recognizer\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# open the file\n",
    "with sr.AudioFile(filename) as source:\n",
    "    # listen for the data (load audio to memory)\n",
    "    audio_data = r.record(source)\n",
    "    # recognize (convert from speech to text)\n",
    "    text = r.recognize_google(audio_data, language=\"nl-BE\")\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
