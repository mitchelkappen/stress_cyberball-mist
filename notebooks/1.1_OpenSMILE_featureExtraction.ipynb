{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Feature Extraction using OpenSMILE (GeMapsv01b + ComParE config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "import traceback\n",
    "from multiprocess import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# opensmile\n",
    "import torchaudio\n",
    "import opensmile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure user\n",
    "user = \"jonas\"\n",
    "extracted_feats = False\n",
    "\n",
    "if user.lower() == \"jonas\":\n",
    "    BASE_PATH = Path(\"/media/uz_study/EEGstudy2_idlab_cloud\")\n",
    "elif user.lower() == \"mitchel\":\n",
    "    #     BASE_PATH = Path(\"Z:/shares/ghep_lab/2021_VanhollebekeKappen_EEGStudy2_MIST_Cyberball_Audio/\")\n",
    "    BASE_PATH = Path(\"D:/Data/EEG_Study_2\")\n",
    "DATA_PATH = BASE_PATH.joinpath(\"Data/Raw/Audio\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "useful links:\n",
    "* [opensmile config folder](https://github.com/audeering/opensmile/tree/v3.0.0/config)\n",
    "* difference between GeMAPS versions [here](https://github.com/audeering/opensmile/blob/v3.0.0/config/gemaps/CHANGES.txt')\n",
    "\n",
    "**note**: `eGeMAPS` is an _extended_ version of the GeMAPS\n",
    "\n",
    "feature-level`\n",
    "* `Functionals`: global segment based features (1 feature per segment)\n",
    "* `LowLevelDescriptor`: sliding window features (1 feature per window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the feature extraction configs\n",
    "func_gemaps = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.GeMAPSv01b,\n",
    "    feature_level=opensmile.FeatureLevel.Functionals,\n",
    ")\n",
    "\n",
    "lld_gemaps = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.GeMAPSv01b,\n",
    "    feature_level=opensmile.FeatureLevel.LowLevelDescriptors,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helper functions ----------\n",
    "def _parse_feat_df(df_feat: pd.DataFrame, wav_path: Path) -> pd.DataFrame:\n",
    "    df_feat[\"file\"] = df_feat[\"file\"].astype(str)\n",
    "    df_feat[\"fileName\"] = wav_path.name\n",
    "\n",
    "    ppt_id, task = wav_path.parent.name.split(\"_\")[1:3]\n",
    "    df_feat[\"participantNum\"] = int(ppt_id)\n",
    "    df_feat[\"taskType\"] = task\n",
    "    desc_type, exp_phase = wav_path.stem.split(\"_\")[1:3]\n",
    "    # description type [picture or referential]\n",
    "    df_feat[\"descriptionType\"] = desc_type\n",
    "    # experimental phase [baseline, control or stress]\n",
    "    df_feat[\"experimentPhase\"] = exp_phase\n",
    "    return df_feat\n",
    "\n",
    "\n",
    "def _extract_parse_smile_resample(s: opensmile.Smile, wav_path: Path) -> pd.DataFrame:\n",
    "    # Load the wav file and resample to 16kHz\n",
    "    arr_orig_wav_n, fs_orig = torchaudio.load(wav_path, normalize=True)\n",
    "    arr_16khz_n = (\n",
    "        torchaudio.functional.resample(\n",
    "            arr_orig_wav_n, orig_freq=fs_orig, new_freq=16_000\n",
    "        )\n",
    "        .numpy()\n",
    "        .ravel()\n",
    "    )\n",
    "\n",
    "    return _parse_feat_df(\n",
    "        s.process_signal(\n",
    "            signal=arr_16khz_n,\n",
    "            sampling_rate=16_000,\n",
    "            file=str(wav_path),\n",
    "        ).reset_index(drop=False),\n",
    "        wav_path=wav_path,\n",
    "    )\n",
    "\n",
    "\n",
    "def _extract_parse_smile(s: opensmile.Smile, wav_path: Path) -> pd.DataFrame:\n",
    "    return _parse_feat_df(\n",
    "        s.process_file(file=wav_path).reset_index(drop=False), wav_path=wav_path\n",
    "    )\n",
    "\n",
    "\n",
    "def _parse_concat_df(df_conc: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_conc[\"participantNum\"] = df_conc[\"participantNum\"].astype(int)\n",
    "    for c in [\"taskType\", \"descriptionType\", \"experimentPhase\", \"file\", \"fileName\"]:\n",
    "        df_conc[c] = df_conc[c].astype(\"category\")\n",
    "    df_conc[\"start\"] = df_conc[\"start\"].dt.total_seconds()\n",
    "    df_conc[\"end\"] = df_conc[\"end\"].dt.total_seconds()\n",
    "    return df_conc\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the raw WAV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_opensmile(file) -> Tuple[pd.DataFrame, ...]:\n",
    "    return (\n",
    "        _extract_parse_smile(func_gemaps, wav_path=file),\n",
    "        _extract_parse_smile(lld_gemaps, wav_path=file),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1088e05e517c4bb1b3f11da924c62849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not extracted_feats:\n",
    "    arr_files = list(DATA_PATH.glob(\"*_speech/audio_*.wav\"))\n",
    "    print(len(arr_files))\n",
    "\n",
    "    out: List = []\n",
    "    with Pool(processes=8) as pool:\n",
    "        results = pool.imap_unordered(_extract_opensmile, arr_files)\n",
    "        results = tqdm(results, total=len(arr_files))\n",
    "        try:\n",
    "            out = [f for f in results]\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            pool.terminate()\n",
    "        finally:\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "    df_fnc_gemaps = _parse_concat_df(pd.concat([o[0] for o in out], ignore_index=True))\n",
    "    df_lld_gemaps = _parse_concat_df(pd.concat([o[1] for o in out], ignore_index=True))\n",
    "    df_fnc_gemaps.to_parquet(DATA_PATH / \"df_gemaps_func.parquet\", engine=\"fastparquet\")\n",
    "    df_lld_gemaps.to_parquet(DATA_PATH / \"df_gemaps_lld.parquet\", engine=\"fastparquet\")\n",
    "    del df_lld_gemaps, df_fnc_gemaps, out, pool, results, arr_files\n",
    "else:\n",
    "    print(\"Nothing to do\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the 16kHz normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811d5e361c5340fbaa8182092f84ec76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _extract_opensmile_rs(file) -> Tuple[pd.DataFrame, ...]:\n",
    "    return (\n",
    "        _extract_parse_smile_resample(func_gemaps, wav_path=file),\n",
    "        _extract_parse_smile_resample(lld_gemaps, wav_path=file),\n",
    "    )\n",
    "\n",
    "\n",
    "if not extracted_feats:\n",
    "    arr_files = list(DATA_PATH.glob(\"*_speech/audio_*.wav\"))\n",
    "    print(len(arr_files))\n",
    "\n",
    "    out: List = []\n",
    "    with Pool(processes=8) as pool:\n",
    "        results = pool.imap_unordered(_extract_opensmile_rs, arr_files)\n",
    "        results = tqdm(results, total=len(arr_files))\n",
    "        try:\n",
    "            out = [f for f in results]\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            pool.terminate()\n",
    "        finally:\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "    df_fnc_gemaps = _parse_concat_df(pd.concat([o[0] for o in out], ignore_index=True))\n",
    "    df_lld_gemaps = _parse_concat_df(pd.concat([o[1] for o in out], ignore_index=True))\n",
    "    df_fnc_gemaps.to_parquet(\n",
    "        DATA_PATH / \"df_gemaps_func_16khz.parquet\", engine=\"fastparquet\"\n",
    "    )\n",
    "    df_lld_gemaps.to_parquet(\n",
    "        DATA_PATH / \"df_gemaps_lld_16khz.parquet\", engine=\"fastparquet\"\n",
    "    )\n",
    "    del df_lld_gemaps, df_fnc_gemaps, out, pool, results, arr_files\n",
    "else:\n",
    "    print(\"Nothing to do\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyberball-mist-analysis-q8GNNw4Z-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31bf377c878654140f2d2331f84826c0fcd47829d48dae7bb22dff810d172b0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
